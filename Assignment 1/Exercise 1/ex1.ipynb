{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSI 5386 - Natural Language Programming\n",
    "# FALL 2022\n",
    "# Assignement 1 : Corpus analysis and word embeddings\n",
    "# Adrien Heymans and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first step before pre-processing or analysis is to concatenate all the documents together. \n",
    "\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "outfilename = \"mergedFiles.txt\"\n",
    "with open(outfilename, 'wb') as outfile:\n",
    "    for filename in glob.glob('CUAD_v1/full_contract_txt/*.txt'):\n",
    "        if filename == outfilename:\n",
    "            # don't want to copy the output into the output\n",
    "            continue\n",
    "        with open(filename, 'rb') as readfile:\n",
    "            shutil.copyfileobj(readfile, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have the merged file, we need want to save it into a value that is a very long list of string\n",
    "\n",
    "from nltk.tokenize import (TreebankWordTokenizer,word_tokenize, wordpunct_tokenize,TweetTokenizer,MWETokenizer)\n",
    "\n",
    "filename=open(\"mergedFiles.txt\",\"r\")\n",
    "tokens = []\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "for line in filename.readlines():\n",
    "    tokens+=tokenizer.tokenize(line)\n",
    "\n",
    "# print(corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4667146\n"
     ]
    }
   ],
   "source": [
    "#The following step is the pre-processing, we need to make sure that the data is as clean as possible. We want to have every word in lower case, remove punctuation,...\n",
    "\n",
    "print(len(tokens))\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(tokens)\n",
    "c =fdist.most_common()\n",
    "with open('tokens.txt', 'w') as f:\n",
    "    for elem in c:\n",
    "        f.write(str(elem))\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
