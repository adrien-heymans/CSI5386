{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mitch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done pre-processing.\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing code. Concatenate all the gold standard results into one file.\n",
    "# So long as the input and gold standard files are in the same order then this is safe.\n",
    "import glob\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Concatenate all the results into one file\n",
    "outFileName = \"data/Result_Files/mergedGoldStandard.txt\"\n",
    "with open(outFileName, 'w', encoding='utf8') as outfile:\n",
    "    for filename in glob.glob('data/Gold_Standard_Files/*.txt'):\n",
    "        if filename == outFileName:\n",
    "            # don't want to copy the output into the output\n",
    "            continue\n",
    "        with open(filename, 'r', encoding='utf8') as readfile:\n",
    "            for line in readfile:\n",
    "                # Remove all the empty lines\n",
    "                if not line.isspace():\n",
    "                    outfile.write(line)\n",
    "\n",
    "# Merge all the input files into one file\n",
    "outFileName = \"data/Input_Files/mergedInputFiles.txt\"\n",
    "with open(outFileName, 'w', encoding='utf8') as outfile:\n",
    "    for filename in glob.glob('data/Input_Files/*.txt'):\n",
    "        if filename == outFileName:\n",
    "            # don't want to copy the output into the output\n",
    "            continue\n",
    "        with open(filename, 'r', encoding='utf8') as readfile:\n",
    "            shutil.copyfileobj(readfile, outfile)\n",
    "\n",
    "print('Done pre-processing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Done testing phase.\n"
     ]
    }
   ],
   "source": [
    "# Import sentence transformer package. More information can be found here: https://www.sbert.net/\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# If you have a cuda capable device we will send the tensors to that\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "\n",
    "# Read in the input files\n",
    "# TODO: Do this over a pandas dataframe\n",
    "readFileName = \"data/Input_Files/mergedInputFiles.txt\"\n",
    "sentences1, sentences2 = [], []\n",
    "with open(readFileName, 'r', encoding='utf8') as readFileName:\n",
    "    for line in readFileName.readlines():\n",
    "        sentences = line.split('\\t')\n",
    "        sentences1.append(sentences[0])\n",
    "        sentences2.append(sentences[1])\n",
    "\n",
    "# The following are SBERT models. More information here: https://www.sbert.net/docs/package_reference/models.html#main-classes\n",
    "model_names = ['all-MiniLM-L6-v2', 'all-mpnet-base-v2', 'paraphrase-mpnet-base-v2', 'distiluse-base-multilingual-cased-v2']\n",
    "# Place the models into an array so we can iterate over them\n",
    "models = [SentenceTransformer(name) for name in model_names]\n",
    "\n",
    "# Run each model on the test data and write their output to a file\n",
    "for name, model in zip(model_names, models):\n",
    "    resultsFileName = f'data/Result_Files/{name}.txt'\n",
    "    #Compute embedding for both lists\n",
    "    embeddings1 = model.encode(sentences1, convert_to_tensor=True, device=device)\n",
    "    embeddings2 = model.encode(sentences2, convert_to_tensor=True, device=device)\n",
    "\n",
    "    #Compute cosine-similarities\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "    # TODO: Confirm taking the absolute value is correct here\n",
    "    # Normalize cosine_scores by taking the absolute value and multiplying by 5 then rounding. Note, the multiplication broadcasts over the tensor.\n",
    "    cosine_scores = torch.round(abs(cosine_scores) * 5)\n",
    "\n",
    "    with open(resultsFileName, 'w', encoding='utf8') as outfile:\n",
    "        for i in range(len(sentences1)):\n",
    "            outfile.writelines(str(int(cosine_scores[i][i].item())) + '\\n')\n",
    "\n",
    "print('Done testing phase.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training doc2Vec model.\n"
     ]
    }
   ],
   "source": [
    "# This is the doc2Vec model. It comes from a different package so it requires some different steps.\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "dataset = api.load(\"text8\")\n",
    "data = [d for d in dataset]\n",
    "def tagged_document(list_of_list_of_words):\n",
    "   for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "      yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "data_for_training = list(tagged_document(data))\n",
    "doc2Vec_model = gensim.models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=30)\n",
    "doc2Vec_model.build_vocab(data_for_training)\n",
    "doc2Vec_model.train(data_for_training, total_examples=doc2Vec_model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "print('Done training doc2Vec model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "tensor([4., 4., 3.,  ..., 4., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "index = 0\n",
    "cosine_scores = torch.zeros(len(sentences1))\n",
    "\n",
    "for sent_1, sent_2 in zip(sentences1, sentences2):\n",
    "    vec1 = word_tokenize(sent_1)\n",
    "    vec2 = word_tokenize(sent_2)\n",
    "    cos_distance = doc2Vec_model.similarity_unseen_docs(vec1, vec2)\n",
    "\n",
    "    # Append to the score tensor\n",
    "    cosine_scores[index] = cos_distance.item()\n",
    "    index += 1\n",
    "\n",
    "cosine_scores = torch.round(abs(cosine_scores) * 5)\n",
    "print(cosine_scores)\n",
    "resultsFileName = 'data/Result_Files/word2vec.txt'\n",
    "with open(resultsFileName, 'w', encoding='utf8') as outfile:\n",
    "    for i in range(len(sentences1)):\n",
    "        outfile.writelines(str(int(cosine_scores[i].item())) + '\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}