{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1665919637447,"user":{"displayName":"Bhavika Sewpal","userId":"17490782924500832028"},"user_tz":240},"id":"OwirtT4lYNzE","outputId":"3d357275-6cc4-4e12-bac0-df235f3e4a1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Done pre-processing.\n"]}],"source":["# Preprocessing code. Concatenate all the gold standard results into one file.\n","# So long as the input and gold standard files are in the same order then this is safe.\n","import glob\n","import shutil\n","import os\n","\n","# Concatenate all the results into one file\n","outFileName = \"data/Result_Files/mergedGoldStandard.txt\"\n","with open(outFileName, 'w', encoding='utf8') as outfile:\n","    for filename in glob.glob('data/Gold_Standard_Files/*.txt'):\n","        if filename == outFileName:\n","            # don't want to copy the output into the output\n","            continue\n","        with open(filename, 'r', encoding='utf8') as readfile:\n","            for line in readfile:\n","                # Remove all the empty lines\n","                # if not line.isspace():\n","                    outfile.write(line)\n","\n","# Merge all the input files into one file\n","outFileName = \"data/Input_Files/mergedInputFiles.txt\"\n","with open(outFileName, 'w', encoding='utf8') as outfile:\n","    for filename in glob.glob('data/Input_Files/*.txt'):\n","        if filename == outFileName:\n","            # don't want to copy the output into the output\n","            continue\n","        with open(filename, 'r', encoding='utf8') as readfile:\n","            shutil.copyfileobj(readfile, outfile)\n","\n","print('Done pre-processing.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GzrXqBX7Zu1q"},"outputs":[],"source":["!pip install nltk\n","!pip install sentence_transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7jfllMz1YNzF","scrolled":true},"outputs":[],"source":["# Import sentence transformer package. More information can be found here: https://www.sbert.net/\n","from sentence_transformers import SentenceTransformer, util\n","import torch\n","\n","# If you have a cuda capable device we will send the tensors to that\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# Assuming that we are on a CUDA machine, this should print a CUDA device:\n","print(device)\n","\n","# Read in the input files\n","# TODO: Do this over a pandas dataframe\n","readFileName = \"data/Input_Files/mergedInputFiles.txt\"\n","sentences1, sentences2 = [], []\n","with open(readFileName, 'r', encoding='utf8') as readFileName:\n","    for line in readFileName.readlines():\n","        sentences = line.split('\\t')\n","        sentences1.append(sentences[0])\n","        sentences2.append(sentences[1])\n","\n","# The following are SBERT models. More information here: https://www.sbert.net/docs/package_reference/models.html#main-classes\n","model_names = ['all-MiniLM-L6-v2', 'all-mpnet-base-v2', 'paraphrase-mpnet-base-v2', 'distiluse-base-multilingual-cased-v2']\n","# Place the models into an array so we can iterate over them\n","models = [SentenceTransformer(name) for name in model_names]\n","\n","# Run each model on the test data and write their output to a file\n","for name, model in zip(model_names, models):\n","    resultsFileName = f'data/Result_Files/{name}.txt'\n","    #Compute embedding for both lists\n","    embeddings1 = model.encode(sentences1, convert_to_tensor=True, device=device)\n","    embeddings2 = model.encode(sentences2, convert_to_tensor=True, device=device)\n","\n","    #Compute cosine-similarities\n","    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n","\n","    # TODO: Confirm taking the absolute value is correct here\n","    # Normalize cosine_scores by taking the absolute value and multiplying by 5 then rounding. Note, the multiplication broadcasts over the tensor.\n","    cosine_scores = torch.round(abs(cosine_scores) * 5)\n","\n","    with open(resultsFileName, 'w', encoding='utf8') as outfile:\n","        for i in range(len(sentences1)):\n","            outfile.writelines(str(int(cosine_scores[i][i].item())) + '\\n')\n","\n","print('Done testing phase.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Js8AkfCvWMz"},"outputs":[],"source":["!pip install gensim\n","!pip install torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7XnL-4fQYNzG","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# This is the doc2Vec model. It comes from a different package so it requires some different steps.\n","import gensim\n","import gensim.downloader as api\n","dataset = api.load(\"text8\")\n","data = [d for d in dataset]\n","def tagged_document(list_of_list_of_words):\n","   for i, list_of_words in enumerate(list_of_list_of_words):\n","      yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n","data_for_training = list(tagged_document(data))\n","doc2Vec_model = gensim.models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=30)\n","doc2Vec_model.build_vocab(data_for_training)\n","doc2Vec_model.train(data_for_training, total_examples=doc2Vec_model.corpus_count, epochs=doc2Vec_model.epochs)\n","\n","print('Done training doc2Vec model.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zLV1HH89YNzH","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","from numpy import dot\n","\n","\n","\n","index = 0\n","cosine_scores = torch.zeros(len(sentences1))\n","\n","def similarity_unseen_docs( model, doc_words1, doc_words2, alpha=0.1, min_alpha=0.0001, steps=5):\n","        \"\"\"\n","        Compute cosine similarity between two post-bulk out of training documents.\n","\n","        Document should be a list of (word) tokens.\n","        \"\"\"\n","        d1 = model.infer_vector(doc_words=doc_words1, alpha=alpha, min_alpha=min_alpha, steps=steps)\n","        d2 = model.infer_vector(doc_words=doc_words2, alpha=alpha, min_alpha=min_alpha, steps=steps)\n","        return dot(gensim.matutils.unitvec(d1), gensim.matutils.unitvec(d2))\n","\n","\n","for sent_1, sent_2 in zip(sentences1, sentences2):\n","    vec1 = word_tokenize(sent_1)\n","    vec2 = word_tokenize(sent_2)\n","    #cos_distance = doc2Vec_model.similarity_unseen_docs(vec1, vec2)\n","    cos_distance = similarity_unseen_docs(doc2Vec_model, vec1, vec2)\n","\n","    # Append to the score tensor\n","    cosine_scores[index] = cos_distance.item()\n","    index += 1\n","\n","cosine_scores = torch.round(abs(cosine_scores) * 5)\n","print(cosine_scores)\n","resultsFileName = 'data/Result_Files/word2vec.txt'\n","with open(resultsFileName, 'w', encoding='utf8') as outfile:\n","    for i in range(len(sentences1)):\n","        outfile.writelines(str(int(cosine_scores[i].item())) + '\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6S_AVJFfXXQ"},"outputs":[],"source":["filenames = ['all-MiniLM-L6-v2', 'all-mpnet-base-v2', 'paraphrase-mpnet-base-v2', 'distiluse-base-multilingual-cased-v2','word2vec']\n","\n","goldStandard = []\n","goldStandardFileName = \"data/Result_Files/mergedGoldStandard.txt\"\n","with open(goldStandardFileName, 'r') as readfile:\n","    for line in readfile.readlines():\n","        goldStandard.append(line.strip())\n","\n","print(goldStandard)\n","\n","\n","for files in filenames:\n","    modelResults = []\n","    modelFileName = f\"data/Result_Files/{files}.txt\"\n","    with open(modelFileName, 'r') as readfile:\n","        for line in readfile.readlines():\n","            modelResults.append(line.strip())\n","    \n","    # We now have removed from the model results the line that also do not exist in the merged gold standard \n","    with open(\"data/Result_Files/\"+files+\"_cleaned.txt\", \"w\") as txt_file:\n","        index = 0\n","        for elem in modelResults:\n","            if goldStandard[index]=='':\n","                txt_file.write(\"\\n\")\n","            else:\n","                txt_file.write(elem+ \"\\n\") # works with any number of elements in a line\n","            index=index+1\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}}},"nbformat":4,"nbformat_minor":0}
