{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSI 5386 - Natural Language Programming\n",
    "# FALL 2022\n",
    "# Assignement 1 : Corpus analysis and word embeddings\n",
    "# CHATTERJEE Micthell, HEYMANS Adrien, SEWPAL Bhavika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step before pre-processing or analysis is to concatenate all the documents together. (Only needed to be executed once)\n",
    "\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "outfilename = \"data/mergedFiles.txt\"\n",
    "with open(outfilename, 'wb') as outfile:\n",
    "    for filename in glob.glob('CUAD_v1/full_contract_txt/*.txt'):\n",
    "        if filename == outfilename:\n",
    "            # don't want to copy the output into the output\n",
    "            continue\n",
    "        with open(filename, 'rb') as readfile:\n",
    "            shutil.copyfileobj(readfile, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe to store our results\n",
    "import pandas as pd\n",
    "results_table = pd.DataFrame(columns=[\"Name\",\"Result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following step is the pre-processing, we need to make sure that the data is as clean as possible. We want to have every word in lower case, remove punctuation,...\n",
    "\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import (TreebankWordTokenizer,word_tokenize, wordpunct_tokenize,TweetTokenizer,MWETokenizer)\n",
    "\n",
    "filename=open(\"data/mergedFiles.txt\",\"r\")\n",
    "tokens = []\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "for line in filename.readlines():\n",
    "    #putting everything to lowercase\n",
    "    line_lowerCase = line.lower()\n",
    "    tokens+=tokenizer.tokenize(line_lowerCase)\n",
    "with open('outputs/output.txt', 'w') as f:\n",
    "    f.write(str(tokens))\n",
    "\n",
    "    \n",
    "fdist = FreqDist(tokens)\n",
    "types =fdist.most_common()\n",
    "\n",
    "frequenceIsOne =0\n",
    "with open('outputs/tokens.txt', 'w') as f:\n",
    "    for elem in types:\n",
    "        if elem[1]==1:\n",
    "            frequenceIsOne=frequenceIsOne+1\n",
    "        f.write(str(elem))\n",
    "\n",
    "\n",
    "# print(\"Number of tokens : \"+str(len(tokens)))\n",
    "# print(\"Number of types : \"+str(len(types)))\n",
    "# print(\"Type/Token ratio : \"+str(len(types)/len(tokens)))\n",
    "# print(\"Tokens appeared only once : \"+str(frequenceIsOne))\n",
    "\n",
    "#Addinf results to result table\n",
    "tp = {\"Name\":\"# of tokens (b)\",\"Result\":str(len(tokens))}\n",
    "results_table = results_table.append(tp,ignore_index=True)\n",
    "tp = {\"Name\":\"# of types (b)\",\"Result\":str(len(types))}\n",
    "results_table = results_table.append(tp,ignore_index=True)\n",
    "tp = {\"Name\":\"type/token ratio (b)\",\"Result\":str(len(types)/len(tokens))}\n",
    "results_table = results_table.append(tp,ignore_index=True)\n",
    "tp = {\"Name\":\"tokens appeared only once (d)\",\"Result\":str(frequenceIsOne)}\n",
    "results_table = results_table.append(tp,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now look into the number of tokens when we remove the punctuation\n",
    "from secrets import token_urlsafe\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# The following tokenzier will remove punctuation\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "filename=open(\"data/mergedFiles.txt\",\"r\")\n",
    "tokensWithoutPunctuation = []\n",
    "\n",
    "\n",
    "for line in filename.readlines():\n",
    "    #putting everything to lowercase\n",
    "    line_lowerCase = line.lower()    \n",
    "    tokensWithoutPunctuation+=tokenizer.tokenize(line_lowerCase)\n",
    "\n",
    "#The last thing to do is to remove the page division character, when we look at the tokens, we noticed that that there are a lot of substring of \"____\", we want to remove these \n",
    "tokensWithoutPunctuation = [item for item in tokensWithoutPunctuation if \"___\" not in item]\n",
    "\n",
    "#Saving tokens to a file\n",
    "with open('outputs/tokens_without_punctuation.txt', 'w') as f:\n",
    "    f.write(str(tokensWithoutPunctuation))\n",
    "\n",
    "\n",
    "fdist = FreqDist(tokensWithoutPunctuation)\n",
    "types =fdist.most_common()\n",
    "\n",
    "frequenceIsOne = 0\n",
    "\n",
    "\n",
    "with open('outputs/tokens_without_punctuation_count.txt', 'w') as f:\n",
    "    for elem in types:\n",
    "        if elem[1]==1:\n",
    "            frequenceIsOne=frequenceIsOne+1\n",
    "        f.write(str(elem))\n",
    "\n",
    "\n",
    "# print(\"Number of words (exluding punctuation) : \"+str(len(types)))\n",
    "# print(\"Type/Token ratio (exluding punctuation) : \"+str(len(types)/len(tokensWithoutPunctuation)))\n",
    "# print(\"The top 3 most frequent words are : \"+str(fdist.most_common(3)))\n",
    "# print(\"Tokens appeared only once : \"+str(frequenceIsOne))\n",
    "\n",
    "\n",
    "#Addinf results to result table\n",
    "tp = {\"Name\":\"# of words (excluding punctuation) (e)\",\"Result\":str(len(types))}\n",
    "results_table = results_table.append(tp,ignore_index=True)\n",
    "tp = {\"Name\":\"type/token ratio (excluding punctuation) (e)\",\"Result\":str(len(types)/len(tokensWithoutPunctuation))}\n",
    "results_table = results_table.append(tp,ignore_index=True)\n",
    "tp = {\"Name\":\"List the top 3 most frequent words and their frequencies (e)\",\"Result\":str(fdist.most_common(3))}\n",
    "results_table = results_table.append(tp,ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are now looking at a tokenizer to remove the punctuation and the stopwords \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# The following tokenzier will remove punctuation\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "filename=open(\"data/mergedFiles.txt\",\"r\")\n",
    "\n",
    "#import the stopwords\n",
    "stopwords_txt = open(\"data/stopwords.txt\",\"r\")\n",
    "stopwords = stopwords_txt.read().splitlines()\n",
    "\n",
    "tokensWithoutPunctuation = []\n",
    "\n",
    "\n",
    "for line in filename.readlines():\n",
    "    #putting everything to lowercase\n",
    "    line_lowerCase = line.lower()\n",
    "    tokensWithoutPunctuation+=tokenizer.tokenize(line_lowerCase)\n",
    "with open('outputs/tokens_without_punctuation.txt', 'w') as f:\n",
    "    f.write(str(tokensWithoutPunctuation))\n",
    "\n",
    "#The last thing to do is to remove the page division character, when we look at the tokens, we noticed that that there are a lot of substring of \"____\", we want to remove these \n",
    "tokensWithoutPunctuation = [item for item in tokensWithoutPunctuation if \"___\" not in item]\n",
    "\n",
    "#removing th stopwords \n",
    "tokensWithoutPunctuationAndStopwords = [] \n",
    "\n",
    "\n",
    "#checking if thw word is a stopword, if it is not, then we are keeping it\n",
    "for word in tokensWithoutPunctuation:\n",
    "     if not(word in stopwords):\n",
    "        tokensWithoutPunctuationAndStopwords.append(word)\n",
    "\n",
    "\n",
    "fdist = FreqDist(tokensWithoutPunctuationAndStopwords)\n",
    "types =fdist.most_common()\n",
    "\n",
    "frequenceIsOne = 0\n",
    "\n",
    "with open('outputs/tokens_without_punctuation_stopwords_count.txt', 'w') as f:\n",
    "    for elem in types:\n",
    "        if elem[1]==1:\n",
    "            frequenceIsOne=frequenceIsOne+1\n",
    "        f.write(str(elem))\n",
    "\n",
    "\n",
    "# print(\"Number of words (exluding punctuation and stopwords) : \"+str(len(types)))\n",
    "# print(\"Type/Token ratio (exluding punctuation and stopwords) : \"+str(len(types)/len(tokensWithoutPunctuationAndStopwords)))\n",
    "# print(\"The top 3 most frequent words are : \"+str(fdist.most_common(3)))\n",
    "# print(\"Tokens appeared only once : \"+str(frequenceIsOne))\n",
    "tp = {\"Name\":\"type/token ratio (excluding punctuation and stopwords) (f)\",\"Result\":str(len(types)/len(tokensWithoutPunctuationAndStopwords))}\n",
    "results_table = results_table.append(tp,ignore_index=True)\n",
    "tp = {\"Name\":\"List the top 3 most frequent words and their frequencies (excluding stopwords) (f)\",\"Result\":str(fdist.most_common(3))}\n",
    "results_table = results_table.append(tp,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are now asked to find the bigrams (excluding stopworda and punctuation) and compute their frequencies \n",
    "\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# The following tokenzier will remove punctuation\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#openning the corpus\n",
    "filename=open(\"data/mergedFiles.txt\",\"r\")\n",
    "\n",
    "#import the stopwords\n",
    "stopwords_txt = open(\"data/stopwords.txt\",\"r\")\n",
    "stopwords = stopwords_txt.read().splitlines()\n",
    "\n",
    "#The list that will store all the bigrams\n",
    "bigrams = []\n",
    "\n",
    "#Tokenizing the sentence and removing stopword.\n",
    "for line in  filename.readlines():\n",
    "    #putting everything to lowercase\n",
    "    \n",
    "    line_lowerCase = line.lower()\n",
    "\n",
    "    #tokenize thr sentence\n",
    "    tokensWithoutPunctuation=tokenizer.tokenize(line_lowerCase)\n",
    "\n",
    "    #removing the stopwords\n",
    "    tokenstokensWithoutPunctuationStopwords = [word for word in tokensWithoutPunctuation if not word in stopwords]\n",
    "\n",
    "    # Removing the \"___\" substring \n",
    "    tokenstokensWithoutPunctuationStopwords = [item for item in tokenstokensWithoutPunctuationStopwords if \"___\" not in item]\n",
    "\n",
    "    #creating the list of bigrams\n",
    "    for elem in list(ngrams(tokenstokensWithoutPunctuationStopwords,2)):\n",
    "        bigrams.append(elem)\n",
    "    #print(ngrams(tokenstokensWithoutPunctuationStopwords,2))\n",
    "    \n",
    "#Using a frequency list to check the frequency of all the bigrams\n",
    "#print(bigrams)\n",
    "bigramsFreq = FreqDist(bigrams)\n",
    "typesBigrams =bigramsFreq.most_common()\n",
    "#print(\"The top 3 most common bigrams are : \"+str(bigramsFreq.most_common(3)))\n",
    "\n",
    "#saving the bigrams in a file\n",
    "with open('outputs/bigrams.txt', 'w') as f:\n",
    "    for elem in typesBigrams:\n",
    "        f.write(str(elem))\n",
    "#Adding to the result table\n",
    "tp = {\"Name\":\"List the top 3 most frequent bigrams and their frequencies (g)\",\"Result\":bigramsFreq.most_common(3)}\n",
    "results_table = results_table.append(tp,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the results table as a csv file\n",
    "results_table.to_csv(\"results/results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
