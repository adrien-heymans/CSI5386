{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Import sentence transformer package. More information can be found here: https://www.sbert.net/\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done pre-processing training and test data.\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the test data code. Concatenate all the gold standard results into one file.\n",
    "# So long as the input and gold standard files are in the same order then this is safe.\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Concatenate all the results into one file\n",
    "outFileName = \"data/Test_Data/Result_Files/mergedGoldStandard.txt\"\n",
    "with open(outFileName, 'w', encoding='utf8') as outfile:\n",
    "    for filename in glob.glob('data/Test_Data/Gold_Standard_Files/*.txt'):\n",
    "        if filename == outFileName:\n",
    "            # don't want to copy the output into the output\n",
    "            continue\n",
    "        with open(filename, 'r', encoding='utf8') as readfile:\n",
    "            for line in readfile:\n",
    "                # Remove all the empty lines\n",
    "                if not line.isspace():\n",
    "                    outfile.write(line)\n",
    "\n",
    "# Merge all the input files into one file\n",
    "outFileName = \"data/Test_Data/Result_Files/mergedInputFiles.txt\"\n",
    "with open(outFileName, 'w', encoding='utf8') as outfile:\n",
    "    for filename in glob.glob('data/Test_Data/Input_Files/*.txt'):\n",
    "        if filename == outFileName:\n",
    "            # don't want to copy the output into the output\n",
    "            continue\n",
    "        with open(filename, 'r', encoding='utf8') as readfile:\n",
    "            shutil.copyfileobj(readfile, outfile)\n",
    "\n",
    "\n",
    "# Note that we need to modify the gold standard files slightly so that the values are compressed to a range 0-1.\n",
    "# As they are currently on a scale from 0-5 we need only divide them by 5. Rounded to 5 decimal places.\n",
    "\n",
    "# Concatenate all the results into one file\n",
    "outFileName = \"data/Training_Data/Result_Files/mergedGoldStandard.txt\"\n",
    "with open(outFileName, 'w', encoding='utf8') as outfile:\n",
    "    for filename in glob.glob('data/Training_Data/Gold_Standard_Files/*.txt'):\n",
    "        with open(filename, 'r', encoding='utf8') as readfile:\n",
    "            for line in readfile:\n",
    "                # Remove all the empty lines\n",
    "                if not line.isspace():\n",
    "                    # Convert the string to a double and then round it to the nearest integer\n",
    "                    val = str(round((float(line) / 5), 5)) + \"\\n\"\n",
    "                    outfile.write(val)\n",
    "\n",
    "# Merge all the input files into one file\n",
    "outFileName = \"data/Training_Data/Result_Files/mergedInputFiles.txt\"\n",
    "with open(outFileName, 'w', encoding='utf8') as outfile:\n",
    "    for filename in glob.glob('data/Test_Data/Input_Files/*.txt'):\n",
    "        with open(filename, 'r', encoding='utf8') as readfile:\n",
    "            shutil.copyfileobj(readfile, outfile)\n",
    "\n",
    "print('Done pre-processing training and test data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# If you have a cuda capable device we will send the tensors to that\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "\n",
    "# Read in the test data into vectors.\n",
    "readFileName = \"data/Test_data/Result_Files/mergedInputFiles.txt\"\n",
    "sentences1, sentences2 = [], []\n",
    "with open(readFileName, 'r', encoding='utf8') as readFileName:\n",
    "    for line in readFileName.readlines():\n",
    "        sentences = line.split('\\t')\n",
    "        sentences1.append(sentences[0])\n",
    "        sentences2.append(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Reading the training data into vectors.\n",
    "from sentence_transformers import SentencesDataset, InputExample, losses\n",
    "\n",
    "training_sentences_1, training_sentences_2, training_labels = [], [], []\n",
    "\n",
    "# Now we read in the sentences and their labels\n",
    "# Read in the input files\n",
    "readFileName = \"data/Training_Data/Result_Files/mergedInputFiles.txt\"\n",
    "with open(readFileName, 'r', encoding='utf8') as readFileName:\n",
    "    for line in readFileName.readlines():\n",
    "        sentences = line.split('\\t')\n",
    "        training_sentences_1.append(sentences[0])\n",
    "        training_sentences_2.append(sentences[1])\n",
    "\n",
    "readFileName = \"data/Training_Data/Result_Files/mergedGoldStandard.txt\"\n",
    "with open(readFileName, 'r', encoding='utf8') as readFileName:\n",
    "    for line in readFileName.readlines():\n",
    "        training_labels.append(float(line))\n",
    "\n",
    "train_examples = []\n",
    "for sent_1, sent_2, label in zip(training_sentences_1, training_sentences_2, training_labels):\n",
    "    train_examples.append(InputExample(texts=[sent_1, sent_2], label=label))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# In this module we, will train one of the transformer models and leave the other in its pre-trained state.\n",
    "# We train this model using the following steps: https://www.sbert.net/examples/training/sts/README.html, https://www.sbert.net/docs/training/overview.html\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# The following are SBERT models. More information here: https://www.sbert.net/docs/package_reference/models.html#main-classes\n",
    "model_name = 'distiluse-base-multilingual-cased-v2'\n",
    "# Place the models into an array so we can iterate over them\n",
    "models = {'With_Training': SentenceTransformer(model_name), 'Without_Training': SentenceTransformer(model_name)}\n",
    "\n",
    "\n",
    "# Train the model\n",
    "training_model = models['With_Training']\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=5)\n",
    "train_loss = losses.CosineSimilarityLoss(training_model)\n",
    "\n",
    "training_model.fit(train_objectives=[(train_dataloader, train_loss)], show_progress_bar=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.26 GiB (GPU 0; 4.00 GiB total capacity; 2.34 GiB already allocated; 0 bytes free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [6], line 13\u001B[0m\n\u001B[0;32m      9\u001B[0m cosine_scores \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mcos_sim(embeddings1, embeddings2)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# TODO: Confirm taking the absolute value is correct here\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Normalize cosine_scores by taking the absolute value and multiplying by 5 then rounding. Note, the multiplication broadcasts over the tensor.\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m cosine_scores \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mround(\u001B[38;5;28mabs\u001B[39m(cosine_scores) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m5\u001B[39m)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(resultsFileName, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf8\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m outfile:\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(sentences1)):\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 1.26 GiB (GPU 0; 4.00 GiB total capacity; 2.34 GiB already allocated; 0 bytes free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Run each model on the test data and write their output to a file\n",
    "for key, model in models.items():\n",
    "    resultsFileName = f'data/Result_Files/{key}_{model_name}.txt'\n",
    "    #Compute embedding for both lists\n",
    "    embeddings1 = model.encode(sentences1, convert_to_tensor=True, device=device)\n",
    "    embeddings2 = model.encode(sentences2, convert_to_tensor=True, device=device)\n",
    "\n",
    "    #Compute cosine-similarities\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "    # TODO: Confirm taking the absolute value is correct here\n",
    "    # Normalize cosine_scores by taking the absolute value and multiplying by 5 then rounding. Note, the multiplication broadcasts over the tensor.\n",
    "    cosine_scores = torch.round(abs(cosine_scores) * 5)\n",
    "\n",
    "    with open(resultsFileName, 'w', encoding='utf8') as outfile:\n",
    "        for i in range(len(sentences1)):\n",
    "            outfile.writelines(str(int(cosine_scores[i][i].item())) + '\\n')\n",
    "\n",
    "print('Done testing phase.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}