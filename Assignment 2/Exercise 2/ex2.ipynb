{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Import sentence transformer package. More information can be found here: https://www.sbert.net/\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 15, 28, 34, 40, 41, 47, 48, 61, 65, 80, 87, 92, 102, 107, 110, 116, 120, 121, 132, 136, 139, 141, 142, 155, 159, 175, 184, 189, 193, 202, 209, 212, 215, 227, 232, 236, 239, 242, 249, 254, 258, 271, 272, 286, 291, 292, 298, 305, 313, 314, 315, 318, 322, 325, 334, 337, 352, 356, 359, 367, 370, 372, 376, 385, 389, 391, 396, 400, 410, 411, 415, 428, 429, 432, 433, 436, 445, 447, 450, 470, 471, 476, 490, 494, 496, 502, 516, 521, 535, 537, 548, 549, 567, 571, 581, 582, 583, 599, 603, 607, 615, 616, 628, 629, 639, 641, 643, 648, 650, 659, 660, 662, 664, 666, 675, 678, 682, 683, 684, 694, 695, 704, 705, 713, 718, 721, 723, 736, 737, 742, 743, 745, 749, 750, 761, 768, 770, 778, 779, 784, 787, 806, 829, 831, 854, 855, 861, 862, 868, 869, 871, 875, 877, 879, 882, 887, 889, 892, 901, 908, 909, 916, 921, 925, 937, 945, 947, 950, 955, 961, 965, 966, 975, 976, 987, 990, 998, 1012, 1013, 1014, 1046, 1052, 1062, 1063, 1065, 1066, 1072, 1077, 1080, 1088, 1099, 1106, 1107, 1111, 1127, 1135, 1150, 1153, 1156, 1162, 1169, 1178, 1179, 1196, 1197, 1199, 1205, 1209, 1216, 1218, 1223, 1225, 1228, 1244, 1250, 1265, 1267, 1305, 1306, 1326, 1327, 1335, 1356, 1360, 1365, 1383, 1389, 1390, 1392, 1397, 1404, 1407, 1416, 1437, 1440, 1445, 1449, 1452, 1453, 1465, 1467, 1477, 1481, 1482, 1490, 1511, 1512, 1537, 1542, 1555, 1557, 1558, 1568, 1584, 1587, 1597, 1616, 1624, 1629, 1635, 1639, 1642, 1643, 1648, 1653, 1654, 1675, 1693, 1706, 1707, 1714, 1721, 1724, 1726, 1732, 1735, 1736, 1742, 1747, 1764, 1765, 1770, 1773, 1789, 1793, 1799, 1812, 1813, 1814, 1816, 1827, 1833, 1837, 1840, 1843, 1844, 1845, 1848, 1853, 1859, 1866, 1867, 1881, 1886, 1893, 1897, 1902, 1911, 1926, 1938, 1943, 1945, 1949, 1959, 1973, 1977, 1979, 1981, 1991, 1992, 1999, 2000, 2009, 2027, 2035, 2038, 2040, 2043, 2045, 2067, 2089, 2104, 2110, 2133, 2141, 2144, 2145, 2146, 2151, 2156, 2166, 2173, 2175, 2194, 2199, 2218, 2228, 2233, 2235, 2239, 2240, 2263, 2267, 2276, 2280, 2286, 2287, 2289, 2298, 2304, 2305, 2307, 2323, 2326, 2331, 2340, 2341, 2347, 2348, 2353, 2360, 2361, 2362, 2365, 2374, 2385, 2387, 2396, 2398, 2404, 2411, 2412, 2413, 2420, 2426, 2430, 2448, 2456, 2458, 2459, 2472, 2475, 2476, 2486, 2494, 2499, 2507, 2514, 2515, 2518, 2519, 2536, 2539, 2544, 2546, 2550, 2551, 2577, 2584, 2593, 2595, 2600, 2601, 2602, 2604, 2605, 2608, 2614, 2619, 2634, 2658, 2663, 2667, 2669, 2672, 2680, 2684, 2687, 2690, 2705, 2706, 2708, 2709, 2720, 2723, 2724, 2736, 2740, 2741, 2742, 2743, 2746, 2750, 2753, 2771, 2772, 2774, 2788, 2804, 2806, 2811, 2818, 2824, 2828, 2835, 2836, 2839, 2847, 2849, 2850, 2855, 2857, 2858, 2862, 2866, 2870, 2875, 2876, 2884, 2885, 2895, 2899, 2900, 2902, 2912, 2918, 2924, 2926, 2932, 2940, 2955, 2972, 2975, 2976, 2980, 2984, 2989, 2992, 2994, 3004, 3006, 3012, 3013, 3015, 3019, 3028, 3037, 3038, 3042, 3049, 3064, 3067, 3072, 3079, 3080, 3081, 3084, 3090, 3095, 3096, 3097, 3099, 3102, 3107, 3112, 3114, 3116, 3124, 3126, 3130, 3136, 3138, 3141, 3143, 3147, 3157, 3160, 3162, 3165, 3168, 3174, 3176, 3177, 3183, 3189, 3193, 3197, 3206, 3207, 3210, 3214, 3218, 3221, 3233, 3237, 3238, 3239, 3248, 3257, 3260, 3266, 3278, 3279, 3280, 3281, 3284, 3289, 3294, 3304, 3305, 3306, 3321, 3324, 3326, 3329, 3333, 3334, 3335, 3340, 3346, 3350, 3353, 3359, 3361, 3366, 3370, 3372, 3374, 3375, 3382, 3387, 3390, 3394, 3402, 3405, 3408, 3416, 3422, 3435, 3438, 3439, 3441, 3442, 3444, 3450, 3452, 3453, 3455, 3462, 3469, 3473, 3489, 3499, 3500, 3503, 3505, 3513, 3514, 3525, 3528, 3529, 3533, 3535, 3540, 3542, 3545, 3546, 3553, 3559, 3565, 3572, 3575, 3580, 3582, 3587, 3588, 3592, 3594, 3596, 3604, 3605, 3609, 3616, 3618, 3641, 3643, 3656, 3663, 3664, 3689, 3690, 3692, 3697, 3707, 3728, 3743, 3749, 3753, 3762, 3766, 3777, 3783, 3788, 3794, 3798, 3802, 3803, 3812, 3814, 3817, 3818, 3820, 3822, 3831, 3836, 3842, 3844, 3845, 3855, 3860, 3861, 3865, 3874, 3876, 3894, 3899, 3900, 3914, 3915, 3918, 3919, 3929, 3946, 3950, 3952, 3970, 3971, 3980, 3987, 3988, 3991, 3994, 3995, 3996, 4003, 4008, 4010, 4013, 4017, 4021, 4049, 4050, 4053, 4058, 4060, 4065, 4066, 4070, 4074, 4086, 4106, 4108, 4126, 4129, 4131, 4136, 4144, 4166, 4183, 4184, 4192, 4195, 4196, 4224, 4254, 4264, 4269, 4291, 4293, 4302, 4310, 4317, 4351, 4354, 4362, 4368, 4375, 4379, 4382, 4389, 4413, 4435, 4439, 4453, 4458, 4469, 4485, 4505, 4510, 4511, 4525, 4535, 4543, 4548, 4581, 4589, 4591, 4597, 4615, 4619, 4620, 4630, 4632, 4640, 4653, 4656, 4712, 4725, 4749, 4778, 4781, 4789, 4790, 4805, 4821, 4832, 4833, 4839, 4874, 4894, 4919, 4926, 4956, 4958, 4967, 4978, 4980, 4984, 4985, 4995, 5004, 5007, 5018, 5040, 5047, 5055, 5063, 5079, 5098, 5106, 5129, 5132, 5138, 5147, 5159, 5182, 5184, 5217, 5241, 5245, 5269, 5279, 5281, 5331, 5335, 5341, 5344, 5354, 5358, 5373, 5376, 5379, 5393, 5413, 5418, 5419, 5429, 5431, 5434, 5454, 5464, 5470, 5480, 5483, 5495, 5498, 5509, 5512, 5520, 5537, 5576, 5578, 5589, 5605, 5611, 5616, 5629, 5651, 5652, 5659, 5679, 5683, 5694, 5708, 5726, 5744, 5752, 5774, 5799, 5806, 5817, 5837, 5845, 5848, 5850, 5874, 5885, 5908, 5912, 5913, 5918, 5932, 5944, 5949, 5978, 5990, 6004, 6027, 6043, 6098, 6102, 6103, 6113, 6118, 6122, 6148, 6167, 6173, 6184, 6185, 6193, 6205, 6268, 6279, 6283, 6285, 6309, 6315, 6320, 6322, 6340, 6371, 6372, 6406, 6412, 6414, 6418, 6428, 6456, 6468, 6515, 6521, 6533, 6583, 6595, 6663, 6685, 6688, 6693, 6714, 6730, 6741, 6751, 6769, 6790, 6795, 6802, 6813, 6816, 6818, 6825, 6846, 6850, 6879, 6909, 6934, 6938, 6939, 6948, 7004, 7020, 7031, 7061, 7068, 7093, 7109, 7112, 7146, 7168, 7184, 7204, 7221, 7227, 7240, 7256, 7260, 7317, 7332, 7334, 7345, 7398, 7422, 7457, 7467, 7478, 7505, 7511, 7515, 7526, 7559, 7581, 7597, 7608, 7609, 7624, 7627, 7632, 7634, 7636, 7642, 7648, 7652, 7683, 7685, 7686, 7687, 7689, 7723, 7725, 7729, 7748, 7756, 7763, 7764, 7771, 7776, 7777, 7778, 7786, 7790, 7791, 7800, 7801, 7809, 7825, 7827, 7830, 7837, 7845, 7849, 7852, 7859, 7861, 7869, 7885, 7898, 7900, 7906, 7920, 7924, 7928, 7930, 7937, 7941, 7944, 7951, 7954, 7957, 7975, 7976, 7977, 7979, 7980, 7981, 7982, 7985, 8002, 8009, 8014, 8015, 8048, 8051, 8055, 8062, 8069, 8071, 8076, 8096, 8099, 8104, 8107, 8110, 8113, 8115, 8119, 8125, 8135, 8145, 8147, 8149, 8174, 8175, 8179, 8184, 8185, 8190, 8198, 8212, 8213, 8236, 8249, 8260, 8275, 8297, 8298, 8306, 8316, 8321, 8323, 8335, 8350, 8357, 8373, 8377, 8387, 8388, 8393, 8400, 8408, 8419, 8422, 8423, 8437, 8441, 8460, 8465, 8477, 8478, 8490, 8506, 8508, 8521, 8526, 8531, 8533, 8534, 8536, 8537, 8540, 8541, 8549, 8553, 8557, 8567, 8578, 8584, 8587, 8596, 8599, 8607, 8636, 8641, 8651, 8660, 8661, 8674, 8707, 8725, 8746, 8776, 8779, 8780, 8784, 8787, 8800, 8805, 8813, 8820, 8836, 8842, 8844, 8846, 8847, 8850, 8851, 8866, 8877, 8882, 8883, 8898, 8902, 8914, 8920, 8923, 8925, 8942, 8952, 8956, 8962, 8972, 8976, 8980, 8983, 8988, 8999, 9000, 9020, 9022, 9048, 9067, 9081, 9085, 9086, 9103, 9110, 9111, 9113, 9115, 9120, 9130, 9151, 9152, 9155, 9164, 9177]\n",
      "9178\n",
      "Done pre-processing training and test data.\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the test data code. Concatenate all the gold standard results into one file.\n",
    "# So long as the input and gold standard files are in the same order then this is safe.\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "indexes = []\n",
    "index = 0\n",
    "\n",
    "# Concatenate all the results into one file\n",
    "outFileName = \"data/Test_Data/Result_Files/mergedGoldStandard.txt\"\n",
    "with open(outFileName, 'w', encoding='utf8') as outfile:\n",
    "    for filename in glob.glob('data/Test_Data/Gold_Standard_Files/*.txt'):\n",
    "        if filename == outFileName:\n",
    "            # don't want to copy the output into the output\n",
    "            continue\n",
    "        with open(filename, 'r', encoding='utf8') as readfile:\n",
    "            for line in readfile:\n",
    "                # Remove all the empty lines\n",
    "                if not line.isspace():\n",
    "                    indexes.append(index)\n",
    "                    outfile.write(line)\n",
    "                index += 1\n",
    "print(indexes)\n",
    "\n",
    "index = 0\n",
    "list_index = 0\n",
    "# Merge all the input files into one file\n",
    "outFileName = \"data/Test_Data/Result_Files/mergedInputFiles.txt\"\n",
    "with open(outFileName, 'w', encoding='utf8') as outfile:\n",
    "    for filename in glob.glob('data/Test_Data/Input_Files/*.txt'):\n",
    "        if filename == outFileName:\n",
    "            # don't want to copy the output into the output\n",
    "            continue\n",
    "        with open(filename, 'r', encoding='utf8') as readfile:\n",
    "            for line in readfile:\n",
    "                if list_index >= len(indexes):\n",
    "                    break\n",
    "                if index == indexes[list_index]:\n",
    "                    outfile.write(line)\n",
    "                    list_index += 1\n",
    "                index += 1\n",
    "\n",
    "print(index)\n",
    "# Note that we need to modify the gold standard files slightly so that the values are compressed to a range 0-1.\n",
    "# As they are currently on a scale from 0-5 we need only divide them by 5. Rounded to 5 decimal places.\n",
    "\n",
    "# Concatenate all the results into one file\n",
    "outFileName = \"data/Training_Data/Result_Files/mergedGoldStandard.txt\"\n",
    "with open(outFileName, 'w', encoding='utf8') as outfile:\n",
    "    for filename in glob.glob('data/Training_Data/Gold_Standard_Files/*.txt'):\n",
    "        with open(filename, 'r', encoding='utf8') as readfile:\n",
    "            for line in readfile:\n",
    "                # Remove all the empty lines\n",
    "                if not line.isspace():\n",
    "                    # Convert the string to a double and then round it to the nearest integer\n",
    "                    val = str(round((float(line) / 5), 5)) + \"\\n\"\n",
    "                    outfile.write(val)\n",
    "\n",
    "# Merge all the input files into one file\n",
    "outFileName = \"data/Training_Data/Result_Files/mergedInputFiles.txt\"\n",
    "with open(outFileName, 'w', encoding='utf8') as outfile:\n",
    "    for filename in glob.glob('data/Test_Data/Input_Files/*.txt'):\n",
    "        with open(filename, 'r', encoding='utf8') as readfile:\n",
    "            shutil.copyfileobj(readfile, outfile)\n",
    "\n",
    "print('Done pre-processing training and test data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1186\n",
      "cuda:0\n",
      "1186\n"
     ]
    }
   ],
   "source": [
    "gold_standard = []\n",
    "readFileName = \"data/Test_Data/Result_Files/mergedGoldStandard.txt\"\n",
    "with open(readFileName, 'r', encoding='utf8') as readFileName:\n",
    "    for line in readFileName.readlines():\n",
    "        gold_standard.append(int(line))\n",
    "print(len(gold_standard))\n",
    "\n",
    "# If you have a cuda capable device we will send the tensors to that\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "\n",
    "# Read in the test data into vectors.\n",
    "readFileName = \"data/Test_data/Result_Files/mergedInputFiles.txt\"\n",
    "sentences1, sentences2 = [], []\n",
    "with open(readFileName, 'r', encoding='utf8') as readFileName:\n",
    "    for line in readFileName.readlines():\n",
    "        sentences = line.split('\\t')\n",
    "        sentences1.append(sentences[0])\n",
    "        sentences2.append(sentences[1])\n",
    "\n",
    "print(len(sentences1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Reading the training data into vectors.\n",
    "from sentence_transformers import SentencesDataset, InputExample, losses\n",
    "\n",
    "training_sentences_1, training_sentences_2, training_labels = [], [], []\n",
    "\n",
    "# Now we read in the sentences and their labels\n",
    "# Read in the input files\n",
    "readFileName = \"data/Training_Data/Result_Files/mergedInputFiles.txt\"\n",
    "with open(readFileName, 'r', encoding='utf8') as readFileName:\n",
    "    for line in readFileName.readlines():\n",
    "        sentences = line.split('\\t')\n",
    "        training_sentences_1.append(sentences[0])\n",
    "        training_sentences_2.append(sentences[1])\n",
    "\n",
    "readFileName = \"data/Training_Data/Result_Files/mergedGoldStandard.txt\"\n",
    "with open(readFileName, 'r', encoding='utf8') as readFileName:\n",
    "    for line in readFileName.readlines():\n",
    "        training_labels.append(float(line))\n",
    "\n",
    "train_examples = []\n",
    "for sent_1, sent_2, label in zip(training_sentences_1, training_sentences_2, training_labels):\n",
    "    train_examples.append(InputExample(texts=[sent_1, sent_2], label=label))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# In this module we, will train one of the transformer models and leave the other in its pre-trained state.\n",
    "# We train this model using the following steps: https://www.sbert.net/examples/training/sts/README.html, https://www.sbert.net/docs/training/overview.html\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# The following are SBERT models. More information here: https://www.sbert.net/docs/package_reference/models.html#main-classes\n",
    "model_name = 'distiluse-base-multilingual-cased-v2'\n",
    "# Place the models into an array so we can iterate over them\n",
    "models = {'With_Training': SentenceTransformer(model_name), 'Without_Training': SentenceTransformer(model_name)}\n",
    "\n",
    "\n",
    "# Train the model\n",
    "training_model = models['With_Training']\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=5)\n",
    "train_loss = losses.CosineSimilarityLoss(training_model)\n",
    "\n",
    "training_model.fit(train_objectives=[(train_dataloader, train_loss)], show_progress_bar=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.26 GiB (GPU 0; 4.00 GiB total capacity; 2.34 GiB already allocated; 0 bytes free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [6], line 13\u001B[0m\n\u001B[0;32m      9\u001B[0m cosine_scores \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mcos_sim(embeddings1, embeddings2)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# TODO: Confirm taking the absolute value is correct here\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Normalize cosine_scores by taking the absolute value and multiplying by 5 then rounding. Note, the multiplication broadcasts over the tensor.\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m cosine_scores \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mround(\u001B[38;5;28mabs\u001B[39m(cosine_scores) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m5\u001B[39m)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(resultsFileName, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf8\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m outfile:\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(sentences1)):\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 1.26 GiB (GPU 0; 4.00 GiB total capacity; 2.34 GiB already allocated; 0 bytes free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Run each model on the test data and write their output to a file\n",
    "for key, model in models.items():\n",
    "    resultsFileName = f'data/Result_Files/{key}_{model_name}.txt'\n",
    "    #Compute embedding for both lists\n",
    "    embeddings1 = model.encode(sentences1, convert_to_tensor=True, device=device)\n",
    "    embeddings2 = model.encode(sentences2, convert_to_tensor=True, device=device)\n",
    "\n",
    "    #Compute cosine-similarities\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "    # TODO: Confirm taking the absolute value is correct here\n",
    "    # Normalize cosine_scores by taking the absolute value and multiplying by 5 then rounding. Note, the multiplication broadcasts over the tensor.\n",
    "    cosine_scores = torch.round(abs(cosine_scores) * 5)\n",
    "\n",
    "    with open(resultsFileName, 'w', encoding='utf8') as outfile:\n",
    "        for i in range(len(sentences1)):\n",
    "            outfile.writelines(str(int(cosine_scores[i][i].item())) + '\\n')\n",
    "\n",
    "print('Done testing phase.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extra code for comparing the pre-trained model vs the additionally trained model\n",
    "\n",
    "gold_standard, with_training, without_training = [], [], []\n",
    "\n",
    "readFileName = \"data/Compare/mergedGoldStandard.txt\"\n",
    "with open(readFileName, 'r', encoding='utf8') as readFileName:\n",
    "    for line in readFileName.readlines():\n",
    "        gold_standard.append(int())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}